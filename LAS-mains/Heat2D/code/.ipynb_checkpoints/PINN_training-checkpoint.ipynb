{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "769a1784-5700-4ab3-ad4d-c757e3c0525d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1056366/301632227.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.fixed_uniform = torch.tensor(x_t_init, dtype = torch.float32, device=self.device, requires_grad=True)\n",
      "/tmp/ipykernel_1056366/301632227.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.XTGrid = torch.tensor(copy.deepcopy(fixed_uniform), dtype=torch.float32, requires_grad=True).to(self.device)\n",
      "/tmp/ipykernel_1056366/301632227.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.XTGrid = torch.tensor(copy.deepcopy(fixed_uniform), dtype=torch.float32, requires_grad=True).to(self.device)\n",
      "/tmp/ipykernel_1056366/301632227.py:337: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  XTGrid = torch.tensor(XTGrid, dtype=torch.float32, requires_grad=True).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number = 1000\n",
      "\tIC Loss = 1.5826504230499268\n",
      "\tBC Loss = 0.1788272261619568\n",
      "\tPhysics Loss = 0.005858185235410929\n",
      "\tTraining Loss = 1.7673358917236328\n",
      "\tRelative L2 error (test) = 81.64623975753784\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 442\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m repeat:\n\u001b[1;32m    426\u001b[0m     pinn \u001b[38;5;241m=\u001b[39m PINN(  k \u001b[38;5;241m=\u001b[39m k,\n\u001b[1;32m    427\u001b[0m                   c \u001b[38;5;241m=\u001b[39m c,\n\u001b[1;32m    428\u001b[0m                   t\u001b[38;5;241m=\u001b[39m T,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    439\u001b[0m                   model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../models/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m args\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.model_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39margs\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(args\u001b[38;5;241m.\u001b[39mlayers)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(args\u001b[38;5;241m.\u001b[39mNf)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i),\n\u001b[1;32m    440\u001b[0m                   display_freq \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mdisplay_freq, samp \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mmethod )\n\u001b[0;32m--> 442\u001b[0m     Losses_train, Losses_rel_l2 \u001b[38;5;241m=\u001b[39m pinn\u001b[38;5;241m.\u001b[39mTrain(args\u001b[38;5;241m.\u001b[39mepochs, weights \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m# initial, boundary, residual\u001b[39;00m\n\u001b[1;32m    444\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(Losses_train, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../models/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m args\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.loss_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39margs\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(args\u001b[38;5;241m.\u001b[39mlayers)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(args\u001b[38;5;241m.\u001b[39mNf)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i))\n\u001b[1;32m    445\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(Losses_rel_l2, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../models/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m args\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.rel_l2_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39margs\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(args\u001b[38;5;241m.\u001b[39mlayers)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(args\u001b[38;5;241m.\u001b[39mNf)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i))\n",
      "Cell \u001b[0;32mIn[10], line 335\u001b[0m, in \u001b[0;36mPINN.Train\u001b[0;34m(self, n_iters, weights)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlas\u001b[38;5;241m.\u001b[39mcnt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml_inf\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 335\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml_inf\u001b[38;5;241m.\u001b[39mupdate(cal_domain_grad, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nn)\n\u001b[1;32m    336\u001b[0m         XTGrid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml_inf\u001b[38;5;241m.\u001b[39mXTGrid\n\u001b[1;32m    337\u001b[0m         XTGrid \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(XTGrid, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[10], line 133\u001b[0m, in \u001b[0;36mL_INFSampler.update\u001b[0;34m(self, phy_lf, model)\u001b[0m\n\u001b[1;32m    130\u001b[0m samples \u001b[38;5;241m=\u001b[39m x_data\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 133\u001b[0m     grad \u001b[38;5;241m=\u001b[39m phy_lf(model, samples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    135\u001b[0m         samples \u001b[38;5;241m=\u001b[39m samples \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_size \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msign(grad)\n",
      "Cell \u001b[0;32mIn[10], line 38\u001b[0m, in \u001b[0;36mcal_domain_grad\u001b[0;34m(model, XYTGrid, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m uyy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(outputs\u001b[38;5;241m=\u001b[39muy, inputs\u001b[38;5;241m=\u001b[39mxyt, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones(uy\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;241m.\u001b[39mto(device),allow_unused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m][:,\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     36\u001b[0m lossf \u001b[38;5;241m=\u001b[39m Loss(uxx\u001b[38;5;241m+\u001b[39muyy\u001b[38;5;241m-\u001b[39mut, torch\u001b[38;5;241m.\u001b[39mzeros_like(uxx, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m---> 38\u001b[0m grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(outputs\u001b[38;5;241m=\u001b[39mlossf, \n\u001b[1;32m     39\u001b[0m                            inputs\u001b[38;5;241m=\u001b[39mxyt, \n\u001b[1;32m     40\u001b[0m                            grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones(lossf\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     41\u001b[0m                            create_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     42\u001b[0m                            allow_unused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad\n",
      "File \u001b[0;32m~/anaconda3/envs/dj24/lib/python3.12/site-packages/torch/autograd/__init__.py:412\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    408\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    409\u001b[0m         grad_outputs_\n\u001b[1;32m    410\u001b[0m     )\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 412\u001b[0m     result \u001b[38;5;241m=\u001b[39m _engine_run_backward(\n\u001b[1;32m    413\u001b[0m         t_outputs,\n\u001b[1;32m    414\u001b[0m         grad_outputs_,\n\u001b[1;32m    415\u001b[0m         retain_graph,\n\u001b[1;32m    416\u001b[0m         create_graph,\n\u001b[1;32m    417\u001b[0m         inputs,\n\u001b[1;32m    418\u001b[0m         allow_unused,\n\u001b[1;32m    419\u001b[0m         accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    420\u001b[0m     )\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    423\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    425\u001b[0m     ):\n",
      "File \u001b[0;32m~/anaconda3/envs/dj24/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader \n",
    "import numpy as np\n",
    "from scipy import io\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "def exact_soln(XYT):\n",
    "    x, y, t = XYT[:,0], XYT[:,1], XYT[:,2]\n",
    "    return 3*torch.sin(np.pi*x)*torch.sin(np.pi*y)*torch.exp(-2*(np.pi**2)*t) + torch.sin(3*np.pi*x)*torch.sin(np.pi*y)*torch.exp(-10*(np.pi**2)*t)\n",
    "\n",
    "\n",
    "def stacked_grid(x,y,t):\n",
    "    X, Y, T = torch.meshgrid(x, y, t)\n",
    "    return torch.hstack((X.flatten()[:, None], Y.flatten()[:, None], T.flatten()[:,None])).float()\n",
    "\n",
    "\n",
    "def cal_domain_grad(model, XYTGrid, device):\n",
    "    Loss = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "    xyt = XYTGrid.requires_grad_(True).to(device)\n",
    "    u = model.forward(xyt)\n",
    "    u_grad = torch.autograd.grad(outputs=u, inputs=xyt, grad_outputs=torch.ones(u.shape).to(device), create_graph=True, allow_unused=True)[0]\n",
    "    ux = u_grad[:,0]\n",
    "    uy = u_grad[:,1]\n",
    "    ut = u_grad[:,2]\n",
    "    uxx = torch.autograd.grad(outputs=ux, inputs=xyt, create_graph=True, grad_outputs=torch.ones(ux.shape).to(device),allow_unused=True)[0][:,0]\n",
    "    uyy = torch.autograd.grad(outputs=uy, inputs=xyt, create_graph=True, grad_outputs=torch.ones(uy.shape).to(device),allow_unused=True)[0][:,1]\n",
    "    \n",
    "    lossf = Loss(uxx+uyy-ut, torch.zeros_like(uxx, device=device).float())\n",
    "\n",
    "    grad = torch.autograd.grad(outputs=lossf, \n",
    "                               inputs=xyt, \n",
    "                               grad_outputs=torch.ones(lossf.shape).to(device),\n",
    "                               create_graph = True,\n",
    "                               allow_unused=True)[0]\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "class RADSampler():\n",
    "    def __init__(self, Nf, device, k, c):    \n",
    "        self.device = device\n",
    "        self.k = k\n",
    "        self.c = c\n",
    "        self.Nf = Nf\n",
    "        self.dense_Nf = Nf*1\n",
    "        \n",
    "    def update(self, model):\n",
    "        \n",
    "        x_new = torch.zeros(self.dense_Nf, 2, dtype = torch.float32, device=self.device).uniform_(0, 1)\n",
    "        t_new = torch.zeros(self.dense_Nf, 1, dtype = torch.float32, device=self.device).uniform_(0, 0.1)\n",
    "        x_t_new = torch.concatenate((x_new,t_new), axis = 1)\n",
    "        \n",
    "        XTGrid = torch.tensor(x_t_new, dtype = torch.float32, device=self.device, requires_grad=True) \n",
    "        XTGrid = XTGrid.to(self.device)\n",
    "\n",
    "        xyt = XTGrid.requires_grad_(True).to(self.device)\n",
    "        \n",
    "        u = model.forward(xyt)\n",
    "        u_grad = torch.autograd.grad(outputs=u, inputs=xyt, grad_outputs=torch.ones(u.shape).to(self.device), create_graph=True, allow_unused=True)[0]\n",
    "        ux = u_grad[:,0]\n",
    "        uy = u_grad[:,1]\n",
    "        ut = u_grad[:,2]\n",
    "        uxx = torch.autograd.grad(outputs=ux, inputs=xyt, create_graph=True, grad_outputs=torch.ones(ux.shape).to(self.device),allow_unused=True)[0][:,0]\n",
    "        uyy = torch.autograd.grad(outputs=uy, inputs=xyt, create_graph=True, grad_outputs=torch.ones(uy.shape).to(self.device),allow_unused=True)[0][:,1]\n",
    "        \n",
    "        err = torch.abs((uxx+uyy-ut))\n",
    "        err = (err**self.k)/((err**self.k).mean())+self.c\n",
    "        err_norm = err/(err.sum())\n",
    "        \n",
    "        indice = torch.multinomial(err_norm, self.Nf, replacement = True)\n",
    "        XTGrid = XTGrid[indice]\n",
    "        self.XTGrid = XTGrid\n",
    "        \n",
    "class LASSampler():\n",
    "    def __init__(self, Nf, fixed_uniform, device, L_iter = 1, beta = 0.2, tau = 0.002):\n",
    "        self.Nf = Nf\n",
    "        self.device = device\n",
    "        self.cnt = 0\n",
    "        self.beta = beta\n",
    "        self.tau = tau\n",
    "        self.L_iter = L_iter\n",
    "        self.XTGrid = torch.tensor(copy.deepcopy(fixed_uniform), dtype=torch.float32, requires_grad=True).to(self.device)\n",
    "\n",
    "    def update(self, phy_lf, model):\n",
    "\n",
    "        x_new = torch.zeros(self.Nf, 2, dtype = torch.float32, device=self.device).uniform_(0, 1)\n",
    "        t_new = torch.zeros(self.Nf, 1, dtype = torch.float32, device=self.device).uniform_(0, 0.1)\n",
    "        x_t_new = torch.concatenate((x_new,t_new), axis = 1)\n",
    "        self.XTGrid = x_t_new\n",
    "\n",
    "        x_data = self.XTGrid\n",
    "        samples = x_data.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        for t in range(1, self.L_iter + 1):\n",
    "            grad = phy_lf(model, samples, self.device)\n",
    "            scaler = torch.sqrt(torch.sum((grad+1e-16)**2, axis = 1)).reshape(-1,1)\n",
    "            grad = grad/scaler\n",
    "            with torch.no_grad():\n",
    "                samples = samples + self.tau * grad + self.beta*torch.sqrt(torch.tensor(2 * self.tau, device=self.device)) * torch.randn(samples.shape, device=self.device)\n",
    "                samples[:, 0] = torch.clamp(samples[:, 0], min=0, max=1) \n",
    "                samples[:, 1] = torch.clamp(samples[:, 1], min=0, max=1)\n",
    "                samples[:, 2] = torch.clamp(samples[:, 2], min=0, max=0.1)\n",
    "            samples = samples.clone().detach().requires_grad_(True)\n",
    "        self.XTGrid = samples.detach()\n",
    "\n",
    "class L_INFSampler():\n",
    "    def __init__(self, Nf, device, step_size = 0.05 , n_iter = 20):\n",
    "\n",
    "        self.Nf = Nf\n",
    "        self.device = device\n",
    "        self.step_size = step_size\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def update(self, phy_lf, model):\n",
    "\n",
    "        x_new = torch.zeros(self.Nf, 2, dtype = torch.float32, device=self.device).uniform_(0, 1)\n",
    "        t_new = torch.zeros(self.Nf, 1, dtype = torch.float32, device=self.device).uniform_(0, 0.1)\n",
    "        x_t_new = torch.concatenate((x_new,t_new), axis = 1)\n",
    "        self.XTGrid = x_t_new\n",
    "            \n",
    "        x_data = self.XTGrid\n",
    "        samples = x_data.clone().detach().requires_grad_(True)\n",
    "    \n",
    "        for t in range(1, self.n_iter + 1):\n",
    "            grad = phy_lf(model, samples, self.device)\n",
    "            with torch.no_grad():\n",
    "                samples = samples + self.step_size * torch.sign(grad)\n",
    "                samples[:, 0] = torch.clamp(samples[:, 0], min=0, max=1)  \n",
    "                samples[:, 1] = torch.clamp(samples[:, 1], min=0, max=1)\n",
    "                samples[:, 2] = torch.clamp(samples[:, 2], min=0, max=0.1) \n",
    "            samples = samples.clone().detach().requires_grad_(True)\n",
    "        self.XTGrid = samples.detach()        \n",
    "\n",
    "\n",
    "class R3Sampler(nn.Module):\n",
    "    def __init__(self,Nf, fixed_uniform, device):\n",
    "        super(R3Sampler, self).__init__()\n",
    "        self.Nf = Nf\n",
    "        self.device = device\n",
    "        self.XTGrid = torch.tensor(copy.deepcopy(fixed_uniform), dtype=torch.float32, requires_grad=True).to(self.device)\n",
    "    \n",
    "    def update(self, loss_aver, loss_ele):\n",
    "        with torch.no_grad():\n",
    "            cho_i = loss_ele > loss_aver\n",
    "            cho_i = cho_i.to('cpu')\n",
    "            self.XTGrid = self.XTGrid[cho_i].detach()\n",
    "            need_n_sample = self.Nf-self.XTGrid.shape[0]\n",
    "            x_new = torch.zeros(need_n_sample, 2, dtype = torch.float32, device=self.device).uniform_(0, 1)\n",
    "            t_new = torch.zeros(need_n_sample, 1, dtype = torch.float32, device=self.device).uniform_(0, 0.1)\n",
    "            x_t_new = torch.concatenate((x_new,t_new), axis = 1)\n",
    "            self.XTGrid = torch.concatenate((self.XTGrid, x_t_new), axis = 0)\n",
    "            self.XTGrid = torch.tensor(self.XTGrid, dtype = torch.float32, device=self.device, requires_grad=True)\n",
    "    \n",
    "class PINN(nn.Module):\n",
    "    def __init__(self,k , c , t, exact_XYT, exact_u, LBs, UBs, Layers, N0, Nb, Nf, \n",
    "                 Activation = nn.Tanh(), \n",
    "                 model_name = \"PINN.model\", device = 'cpu',\n",
    "                  display_freq = 100, samp = 'fixed' ):\n",
    "        \n",
    "        super(PINN, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.LBs = torch.tensor(LBs, dtype=torch.float32)\n",
    "        self.UBs = torch.tensor(UBs, dtype=torch.float32)\n",
    "        \n",
    "        self.Layers = Layers\n",
    "        self.in_dim  = Layers[0]\n",
    "        self.out_dim = Layers[-1]\n",
    "        self.Activation = Activation\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        x_init = torch.zeros(Nf, 2, dtype = torch.float32, device=self.device).uniform_(0, 1)\n",
    "        t_init = torch.zeros(Nf, 1, dtype = torch.float32, device=self.device).uniform_(0, 0.1)\n",
    "        x_t_init = torch.concatenate((x_init,t_init), axis = 1)\n",
    "        self.fixed_uniform = torch.tensor(x_t_init, dtype = torch.float32, device=self.device, requires_grad=True)\n",
    "        \n",
    "        self.N0 = N0\n",
    "        self.Nb = Nb\n",
    "        self.Nf = Nf\n",
    "        \n",
    "        self.t = t\n",
    "        self.exact_XYT = exact_XYT\n",
    "        self.exact_u = exact_u.reshape(-1,1)\n",
    "        \n",
    "        self.XT0, self.u0  = self.InitialCondition(self.N0, self.LBs, self.UBs)\n",
    "        self.left, self.right, self.top, self.bottom = self.BoundaryCondition(self.Nb, self.LBs, self.UBs)\n",
    "        \n",
    "        self.XT0 = self.XT0.to(device)\n",
    "        self.u0 = self.u0.to(device) \n",
    "        \n",
    "        self.left = self.left.to(device) \n",
    "        self.right = self.right.to(device)\n",
    "        self.top = self.top.to(device) \n",
    "        self.bottom = self.bottom.to(device)\n",
    "        \n",
    "        self._nn = self.build_model()\n",
    "        self._nn.to(self.device)\n",
    "        self.Loss = torch.nn.MSELoss(reduction='mean')\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.display_freq = display_freq\n",
    "        \n",
    "        self.k = k\n",
    "        self.c = c\n",
    "        self.X_star = self.exact_XYT\n",
    "        self.method = samp\n",
    "        \n",
    "        self.r3_sample = R3Sampler(self.Nf, self.fixed_uniform, device)\n",
    "        self.las = LASSampler(self.Nf, fixed_uniform=self.fixed_uniform, device=self.device, L_iter = 1, beta = 0.2, tau=2e-3)\n",
    "        self.l_inf = L_INFSampler(self.Nf, device=self.device, step_size = 0.05 , n_iter = 20)\n",
    "        self.rad = RADSampler(Nf = self.Nf, device=self.device, k = self.k, c=self.c)\n",
    "        \n",
    "    \n",
    "    def build_model(self):\n",
    "        Seq = nn.Sequential()\n",
    "        for ii in range(len(self.Layers)-1):\n",
    "            this_module = nn.Linear(self.Layers[ii], self.Layers[ii+1])\n",
    "            nn.init.xavier_normal_(this_module.weight)\n",
    "            Seq.add_module(\"Linear\" + str(ii), this_module)\n",
    "            if not ii == len(self.Layers)-2:\n",
    "                Seq.add_module(\"Activation\" + str(ii), self.Activation)\n",
    "        return Seq\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = 2*(x - self.LBs.to(x.device)/((self.UBs - self.LBs).to(x.device))) - 1.0\n",
    "        #print(3.5, x.shape)\n",
    "        return self._nn.forward(x.to(self.device)).reshape(-1)\n",
    "\n",
    "    def InitialCondition(self, N0, LB, UB):\n",
    "        n_per_dim = int(np.round(np.sqrt(N0)))\n",
    "        t_in = torch.tensor([LB[-1]]).float()\n",
    "        x_in = torch.linspace(LB[0], UB[0], n_per_dim).float()\n",
    "        y_in = torch.linspace(LB[1], UB[1], n_per_dim).float()\n",
    "        XYT_in = stacked_grid(x_in, y_in, t_in)\n",
    "        u0 = exact_soln(XYT_in)\n",
    "        return XYT_in, u0\n",
    "\n",
    "    def BoundaryPoints(self,nb, xb, yb, LBs, UBs, where = 'left'):\n",
    "        n_per_dim = int(np.round(np.sqrt(nb)))\n",
    "        if where in ['left', 'right']:\n",
    "            Xb = torch.tensor([xb]).float()\n",
    "            Yb = torch.linspace(LBs[1], UBs[1], n_per_dim).float()\n",
    "        else:\n",
    "            Yb = torch.tensor([yb]).float()\n",
    "            Xb = torch.linspace(LBs[0], UBs[0], n_per_dim).float()\n",
    "        Tb = torch.linspace(LBs[-1], UBs[-1], nb).float()\n",
    "        return stacked_grid(Xb,Yb,Tb)\n",
    "\n",
    "    \n",
    "    def BoundaryCondition(self,Nb, LBs, UBs):\n",
    "        ## left boundary: \n",
    "        ## Choose Nb time instances on x = LB, x = UB, y = LB, and y = UB\n",
    "        nb = int(np.round(Nb/4))\n",
    "        XYTleft = self.BoundaryPoints(nb, LBs[0], LBs[1], LBs, UBs, 'left')\n",
    "        XYTright = self.BoundaryPoints(nb, UBs[0], LBs[1], LBs, UBs, 'right')\n",
    "        XYTtop = self.BoundaryPoints(nb, LBs[0], UBs[1], LBs, UBs, 'top')\n",
    "        XYTbottom = self.BoundaryPoints(nb, LBs[0], LBs[1], LBs, UBs, 'bottom')\n",
    "        return XYTleft, XYTright, XYTtop, XYTbottom\n",
    "    \n",
    "    def ICLoss(self):\n",
    "        uv0_pred = self.forward(self.XT0)\n",
    "        loss = self.Loss(uv0_pred, self.u0.to(self.device))\n",
    "        return loss\n",
    "        \n",
    "    def BCLoss(self):\n",
    "        U_L, U_R, U_T, U_B = self.forward(self.left), self.forward(self.right), self.forward(self.top), self.forward(self.bottom)\n",
    "        ULx, URx, UTx, UBx = exact_soln(self.left).to(self.device), exact_soln(self.right).to(self.device), \\\n",
    "                             exact_soln(self.top).to(self.device), exact_soln(self.bottom).to(self.device)\n",
    "        return self.Loss(U_L, ULx) + self.Loss(U_R, URx) + \\\n",
    "               self.Loss(U_T, UTx) + self.Loss(U_B, UBx)\n",
    "    \n",
    "    def PhysicsLoss(self, XTGrid):\n",
    "        xyt = XTGrid.requires_grad_(True).to(self.device)\n",
    "        u = self.forward(xyt)\n",
    "        u_grad = torch.autograd.grad(outputs=u, inputs=xyt, grad_outputs=torch.ones(u.shape).to(self.device), create_graph=True, allow_unused=True)[0]\n",
    "        ux = u_grad[:,0]\n",
    "        uy = u_grad[:,1]\n",
    "        ut = u_grad[:,2]\n",
    "        uxx = torch.autograd.grad(outputs=ux, inputs=xyt, create_graph=True, grad_outputs=torch.ones(ux.shape).to(self.device),allow_unused=True)[0][:,0]\n",
    "        uyy = torch.autograd.grad(outputs=uy, inputs=xyt, create_graph=True, grad_outputs=torch.ones(uy.shape).to(self.device),allow_unused=True)[0][:,1]\n",
    "\n",
    "        loss2 = (uxx+uyy-ut)**2\n",
    "        loss1 = loss2.mean()\n",
    "             \n",
    "        return loss1, loss2 \n",
    "\n",
    "    def Train(self, n_iters, weights=(1.0,1.0,1.0)):\n",
    "        params = list(self.parameters())\n",
    "        optimizer = optim.Adam(params, lr=1e-3)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 5000, gamma=0.9, last_epoch=-1)\n",
    "        min_loss = 999999.0\n",
    "        Training_Losses = [-10]*n_iters\n",
    "        Test_Losses = []\n",
    "        rel_error = [-10]*(1+n_iters//1000)\n",
    "        \n",
    "        for jj in range(n_iters):\n",
    "            Total_ICLoss = torch.tensor(0.0, dtype = torch.float32, device=self.device, requires_grad = True)\n",
    "            Total_BCLoss = torch.tensor(0.0, dtype = torch.float32, device=self.device, requires_grad = True)\n",
    "            Total_PhysicsLoss = torch.tensor(0.0, dtype = torch.float32, device=self.device, requires_grad = True)\n",
    "            \n",
    "            Total_ICLoss = Total_ICLoss + self.ICLoss()\n",
    "            Total_BCLoss = Total_BCLoss + self.BCLoss()\n",
    "            \n",
    "            if self.method =='r3':\n",
    "                if jj == 0:\n",
    "                    XTGrid = self.r3_sample.XTGrid\n",
    "                    XTGrid = torch.tensor(XTGrid, dtype = torch.float32, device=self.device, requires_grad=True) \n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        self.r3_sample.update(loss1, loss2)\n",
    "                        XTGrid = self.r3_sample.XTGrid\n",
    "                        XTGrid = torch.tensor(XTGrid, dtype = torch.float32, device=self.device, requires_grad=True) \n",
    "                        \n",
    "            elif self.method == 'las':\n",
    "                if jj == 0:\n",
    "                    XTGrid = self.las.XTGrid\n",
    "                    XTGrid = torch.tensor(XTGrid, dtype=torch.float32, requires_grad=True).to(self.device)\n",
    "                else:\n",
    "                    if self.las.cnt % 1 == 0:# 4,6,8,10 cnt = 4, \n",
    "                        self.las.update(cal_domain_grad, self._nn)\n",
    "                    XTGrid = self.las.XTGrid\n",
    "                    XTGrid = torch.tensor(XTGrid, dtype=torch.float32, requires_grad=True).to(self.device)\n",
    "                self.las.cnt += 1\n",
    "            \n",
    "            elif self.method =='l_inf':\n",
    "                    self.l_inf.update(cal_domain_grad, self._nn)\n",
    "                    XTGrid = self.l_inf.XTGrid\n",
    "                    XTGrid = torch.tensor(XTGrid, dtype=torch.float32, requires_grad=True).to(self.device)\n",
    "                \n",
    "            elif self.method =='rad':\n",
    "                    self.rad.update(self._nn)\n",
    "                    XTGrid = self.rad.XTGrid\n",
    "                    XTGrid = torch.tensor(XTGrid, dtype=torch.float32, requires_grad=True).to(self.device)   \n",
    "            \n",
    "            elif self.method =='fixed':\n",
    "                    XTGrid = torch.tensor(self.fixed_uniform, dtype = torch.float32, device=self.device, requires_grad=True) \n",
    "            \n",
    "            elif self.method =='random-r':\n",
    "                    x_new = torch.zeros(self.Nf, 2, dtype = torch.float32, device=self.device).uniform_(0, 1)\n",
    "                    t_new = torch.zeros(self.Nf, 1, dtype = torch.float32, device=self.device).uniform_(0, 0.1)\n",
    "                    x_t_new = torch.concatenate((x_new,t_new), axis = 1)\n",
    "                    XTGrid = torch.tensor(x_t_new, dtype = torch.float32, device=self.device, requires_grad=True) \n",
    "                \n",
    "            optimizer.zero_grad()    \n",
    "            loss1, loss2 = self.PhysicsLoss(XTGrid) # For r3 method, loss2 contains element-wise errors\n",
    "            \n",
    "            Total_PhysicsLoss = Total_PhysicsLoss + loss1\n",
    "            Total_Loss = weights[0]*Total_ICLoss + weights[1]*Total_BCLoss\\\n",
    "                        + weights[2]*Total_PhysicsLoss \n",
    "            \n",
    "            Total_Loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            if Total_Loss < min_loss:\n",
    "                torch.save(self._nn.state_dict(), \"../models/\"+self.method+'_'+str(len(self.Layers)-2)+'_'+str(self.Nf)+'.pt')\n",
    "                min_loss = float(Total_Loss)\n",
    "                    \n",
    "            Training_Losses[jj] = float(Total_Loss)\n",
    "            \n",
    "            if (jj+1) % self.display_freq == 0:\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.forward(self.X_star)\n",
    "                    outputs = outputs.reshape(-1,1)\n",
    "                    re = np.linalg.norm(self.exact_u.cpu()-outputs.cpu().detach()) / np.linalg.norm(self.exact_u.cpu().detach())\n",
    "                    rel_error[int((jj+1)/1000)] = float(re*100)\n",
    "                    \n",
    "                print(\"Iteration Number = {}\".format(jj+1))\n",
    "                print(\"\\tIC Loss = {}\".format(float(Total_ICLoss)))\n",
    "                print(\"\\tBC Loss = {}\".format(float(Total_BCLoss)))\n",
    "                print(\"\\tPhysics Loss = {}\".format(float(Total_PhysicsLoss)))\n",
    "                print(\"\\tTraining Loss = {}\".format(float(Total_Loss)))\n",
    "                print(\"\\tRelative L2 error (test) = {}\".format(float(re*100)))\n",
    "\n",
    "        return Training_Losses, rel_error\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "                \n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--nodes', type=int, default = 128, help='The number of nodes per hidden layer in the neural network')\n",
    "        parser.add_argument('--layers', type=int, default = 8, help='The number of hidden layers in the neural network')\n",
    "        parser.add_argument('--N0', type=int, default = 100, help='The number of points to use on the initial condition')\n",
    "        parser.add_argument('--Nb', type=int, default = 100, help='The number of points to use on the boundary condition')\n",
    "        parser.add_argument('--Nf', type=int, default = 1000, help='The number of collocation points to use')\n",
    "        parser.add_argument('--epochs', type=int, default = 200000, help='The number of epochs to train the neural network')\n",
    "        parser.add_argument('--method', type=str, default='random-r', help='Sampling method') # fixed, random-r, rad, r3, l_inf, las \n",
    "        parser.add_argument('--model-name', type=str, default='PINN_model', help='File name to save the model')\n",
    "        parser.add_argument('--display-freq', type=int, default=1000, help='How often to display loss information')\n",
    "        parser.add_argument('-f')\n",
    "        args = parser.parse_args()\n",
    "\n",
    "\n",
    "        if not os.path.exists(\"../models/\"):\n",
    "            os.mkdir(\"../models/\")\n",
    "            \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        NHiddenLayers = args.layers\n",
    "        \n",
    "        LBs = [0, 0, 0.0]\n",
    "        UBs = [1.0, 1.0, 0.1]\n",
    "\n",
    "        Nx, Ny, Nt = 100, 100, 20\n",
    "        X = torch.linspace(LBs[0], UBs[0], Nx).float()\n",
    "        Y = torch.linspace(LBs[1], UBs[1], Ny).float()\n",
    "        T = torch.linspace(LBs[2], UBs[2], Nt).float()\n",
    "        XYT = stacked_grid(X,Y,T)\n",
    "        Exact_U = exact_soln(XYT)\n",
    "\n",
    "        Layers = [3] + [args.nodes]*NHiddenLayers + [1]\n",
    "        Activation = nn.Tanh()\n",
    "\n",
    "        k = 1\n",
    "        c = 1\n",
    "\n",
    "        repeat = [0, 1, 2, 3, 4]\n",
    "        for i in repeat:\n",
    "            pinn = PINN(  k = k,\n",
    "                          c = c,\n",
    "                          t= T,\n",
    "                          exact_XYT = XYT,\n",
    "                          exact_u = Exact_U,\n",
    "                          Layers = Layers,\n",
    "                          LBs = LBs,\n",
    "                          UBs = UBs,\n",
    "                          N0 = args.N0,\n",
    "                          Nb = args.Nb,\n",
    "                          Nf = args.Nf,\n",
    "                          Activation = Activation,\n",
    "                          device = device,\n",
    "                          model_name = \"../models/\" + args.model_name + \".model_\"+args.method+'_'+str(args.layers)+'_'+str(args.Nf)+'_'+str(i),\n",
    "                          display_freq = args.display_freq, samp = args.method )\n",
    "\n",
    "            Losses_train, Losses_rel_l2 = pinn.Train(args.epochs, weights = (1, 1, 1)) # initial, boundary, residual\n",
    "\n",
    "            torch.save(Losses_train, \"../models/\" + args.model_name + \".loss_\"+args.method+'_'+str(args.layers)+'_'+str(args.Nf)+'_'+str(i))\n",
    "            torch.save(Losses_rel_l2, \"../models/\" + args.model_name + \".rel_l2_\"+args.method+'_'+str(args.layers)+'_'+str(args.Nf)+'_'+str(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
