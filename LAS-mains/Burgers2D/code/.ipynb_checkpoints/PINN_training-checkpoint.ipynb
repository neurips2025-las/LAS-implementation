{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd300e03-8913-484e-8712-d588c06e3cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/dj24/lib/python3.12/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/tmp/ipykernel_1093804/699943700.py:215: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.fixed_uniform = torch.tensor(x_t_init, dtype = torch.float32, device=self.device, requires_grad=True)\n",
      "/tmp/ipykernel_1093804/699943700.py:176: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.XTGrid = torch.tensor(copy.deepcopy(fixed_uniform), dtype=torch.float32, requires_grad=True).to(self.device)\n",
      "/tmp/ipykernel_1093804/699943700.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.XTGrid = torch.tensor(copy.deepcopy(fixed_uniform), dtype=torch.float32, requires_grad=True).to(self.device)\n",
      "/tmp/ipykernel_1093804/699943700.py:357: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  XTGrid = torch.tensor(XTGrid, dtype = torch.float32, device=self.device, requires_grad=True)\n",
      "/tmp/ipykernel_1093804/699943700.py:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.XTGrid = torch.tensor(self.XTGrid, dtype = torch.float32, device=self.device, requires_grad=True)\n",
      "/tmp/ipykernel_1093804/699943700.py:362: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  XTGrid = torch.tensor(XTGrid, dtype = torch.float32, device=self.device, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number = 1000\n",
      "\tIC Loss = 2.2400938178179786e-05\n",
      "\tBC Loss = 9.551125549478456e-05\n",
      "\tPhysics Loss = 0.00011336875468259677\n",
      "\tTraining Loss = 0.00023128095199353993\n",
      "\tRelative L2 error (test) = 0.5758739076554775\n",
      "Iteration Number = 2000\n",
      "\tIC Loss = 7.707754434704839e-07\n",
      "\tBC Loss = 3.4790125482686562e-06\n",
      "\tPhysics Loss = 1.043613792717224e-05\n",
      "\tTraining Loss = 1.4685925634694286e-05\n",
      "\tRelative L2 error (test) = 0.11536726960912347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader \n",
    "import numpy as np\n",
    "from scipy import io\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "Re = np.pi/0.05\n",
    "\n",
    "def exact_soln(XYT):\n",
    "\n",
    "    x, y, t = XYT[:,0], XYT[:,1], XYT[:,2]\n",
    "    c = 0.25/(1 + torch.exp( Re * (-t -4*x + 4*y)/32))\n",
    "    u  = (0.75 - c).reshape(-1,1)\n",
    "    v = (0.75 + c).reshape(-1,1)\n",
    "\n",
    "    return torch.cat((u, v), 1)\n",
    "\n",
    "\n",
    "def stacked_grid(x,y,t):\n",
    "    X, Y, T = torch.meshgrid(x, y, t)\n",
    "    return torch.hstack((X.flatten()[:, None], Y.flatten()[:, None], T.flatten()[:,None])).float()\n",
    "\n",
    "\n",
    "def cal_domain_grad(model, XYTGrid, device):\n",
    "    Loss = torch.nn.MSELoss(reduction='mean')\n",
    "    xyt = XYTGrid.requires_grad_(True).to(device)\n",
    "    uv = model.forward(xyt)\n",
    "    u = uv[:,0]\n",
    "    v = uv[:,1]\n",
    "\n",
    "    u_grad = torch.autograd.grad(outputs=u, inputs=xyt, grad_outputs=torch.ones(u.shape).to(device), create_graph=True, allow_unused=True)[0]\n",
    "    ux = u_grad[:,0]\n",
    "    uy = u_grad[:,1]\n",
    "    ut = u_grad[:,2]\n",
    "    uxx = torch.autograd.grad(outputs=ux, inputs=xyt, create_graph=True, grad_outputs=torch.ones(u.shape).to(device),allow_unused=True)[0][:,0]\n",
    "    uyy = torch.autograd.grad(outputs=uy, inputs=xyt, create_graph=True, grad_outputs=torch.ones(u.shape).to(device),allow_unused=True)[0][:,1]\n",
    "\n",
    "    v_grad = torch.autograd.grad(outputs=v, inputs=xyt, grad_outputs=torch.ones(u.shape).to(device), create_graph=True, allow_unused=True)[0]\n",
    "    vx = v_grad[:,0]\n",
    "    vy = v_grad[:,1]\n",
    "    vt = v_grad[:,2]\n",
    "    vxx = torch.autograd.grad(outputs=vx, inputs=xyt, create_graph=True, grad_outputs=torch.ones(u.shape).to(device),allow_unused=True)[0][:,0]\n",
    "    vyy = torch.autograd.grad(outputs=vy, inputs=xyt, create_graph=True, grad_outputs=torch.ones(u.shape).to(device),allow_unused=True)[0][:,1]\n",
    "\n",
    "    \n",
    "    lossf = Loss(ut + u*ux + v*uy - (1/Re)*(uxx + uyy), torch.zeros_like(uxx, device=device).float()) + \\\n",
    "            Loss(vt + u*vx + v*vy - (1/Re)*(vxx + vyy), torch.zeros_like(vxx, device=device).float())\n",
    "\n",
    "    grad = torch.autograd.grad(outputs=lossf, \n",
    "                               inputs=xyt, \n",
    "                               grad_outputs=torch.ones(lossf.shape).to(device),\n",
    "                               create_graph = True,\n",
    "                               allow_unused=True)[0]\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "class RADSampler():\n",
    "    def __init__(self, Nf, device, k, c):    \n",
    "        self.device = device\n",
    "        self.k = k\n",
    "        self.c = c\n",
    "        self.Nf = Nf\n",
    "        self.dense_Nf = Nf*1\n",
    "        \n",
    "    def update(self, model):\n",
    "        \n",
    "        x_new = torch.zeros(self.dense_Nf, 2, dtype = torch.float32, device=self.device).uniform_(0, 1)\n",
    "        t_new = torch.zeros(self.dense_Nf, 1, dtype = torch.float32, device=self.device).uniform_(0, 1)\n",
    "        x_t_new = torch.concatenate((x_new,t_new), axis = 1)\n",
    "        \n",
    "        XTGrid = torch.tensor(x_t_new, dtype = torch.float32, device=self.device, requires_grad=True) \n",
    "        XTGrid = XTGrid.to(self.device)\n",
    "\n",
    "        \n",
    "        xyt = XTGrid.requires_grad_(True).to(self.device)\n",
    "        uv = model.forward(xyt)\n",
    "        u = uv[:,0]\n",
    "        v = uv[:,1]\n",
    "\n",
    "        u_grad = torch.autograd.grad(outputs=u, inputs=xyt, grad_outputs=torch.ones(u.shape).to(self.device), create_graph=True, allow_unused=True)[0]\n",
    "        ux = u_grad[:,0]\n",
    "        uy = u_grad[:,1]\n",
    "        ut = u_grad[:,2]\n",
    "        uxx = torch.autograd.grad(outputs=ux, inputs=xyt, create_graph=True, grad_outputs=torch.ones(u.shape).to(self.device),allow_unused=True)[0][:,0]\n",
    "        uyy = torch.autograd.grad(outputs=uy, inputs=xyt, create_graph=True, grad_outputs=torch.ones(u.shape).to(self.device),allow_unused=True)[0][:,1]\n",
    "\n",
    "        v_grad = torch.autograd.grad(outputs=v, inputs=xyt, grad_outputs=torch.ones(u.shape).to(self.device), create_graph=True, allow_unused=True)[0]\n",
    "        vx = v_grad[:,0]\n",
    "        vy = v_grad[:,1]\n",
    "        vt = v_grad[:,2]\n",
    "        vxx = torch.autograd.grad(outputs=vx, inputs=xyt, create_graph=True, grad_outputs=torch.ones(u.shape).to(self.device),allow_unused=True)[0][:,0]\n",
    "        vyy = torch.autograd.grad(outputs=vy, inputs=xyt, create_graph=True, grad_outputs=torch.ones(u.shape).to(self.device),allow_unused=True)[0][:,1]\n",
    "\n",
    "        err = torch.abs((ut + u*ux + v*uy - (1/Re)*(uxx + uyy)))+torch.abs((vt + u*vx + v*vy - (1/Re)*(vxx + vyy)))\n",
    "        err = (err**self.k)/((err**self.k).mean())+self.c\n",
    "        err_norm = err/(err.sum())\n",
    "        \n",
    "        indice = torch.multinomial(err_norm, self.Nf, replacement = True)\n",
    "        XTGrid = XTGrid[indice]\n",
    "        self.XTGrid = XTGrid\n",
    "        \n",
    "class LASSampler():\n",
    "    def __init__(self, Nf, fixed_uniform, device, L_iter = 1, beta = 0.2, tau = 0.002):\n",
    "        self.Nf = Nf\n",
    "        self.device = device\n",
    "        self.cnt = 0\n",
    "        self.beta = beta\n",
    "        self.tau = tau\n",
    "        self.L_iter = L_iter\n",
    "        self.XTGrid = torch.tensor(copy.deepcopy(fixed_uniform), dtype=torch.float32, requires_grad=True).to(self.device)\n",
    "\n",
    "    def update(self, phy_lf, model):\n",
    "\n",
    "        x_new = torch.zeros(self.Nf, 2, dtype = torch.float32, device=self.device).uniform_(0, 1)\n",
    "        t_new = torch.zeros(self.Nf, 1, dtype = torch.float32, device=self.device).uniform_(0, 1)\n",
    "        x_t_new = torch.concatenate((x_new,t_new), axis = 1)\n",
    "        self.XTGrid = x_t_new\n",
    "\n",
    "        x_data = self.XTGrid\n",
    "        samples = x_data.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        for t in range(1, self.L_iter + 1):\n",
    "            grad = phy_lf(model, samples, self.device)\n",
    "            scaler = torch.sqrt(torch.sum((grad+1e-16)**2, axis = 1)).reshape(-1,1)\n",
    "            grad = grad/scaler\n",
    "            with torch.no_grad():\n",
    "                samples = samples + self.tau * grad + self.beta*torch.sqrt(torch.tensor(2 * self.tau, device=self.device)) * torch.randn(samples.shape, device=self.device)\n",
    "                samples[:, 0] = torch.clamp(samples[:, 0], min=0, max=1) \n",
    "                samples[:, 1] = torch.clamp(samples[:, 1], min=0, max=1)\n",
    "                samples[:, 2] = torch.clamp(samples[:, 2], min=0, max=1)\n",
    "            samples = samples.clone().detach().requires_grad_(True)\n",
    "        self.XTGrid = samples.detach()\n",
    "\n",
    "class L_INFSampler():\n",
    "    def __init__(self, Nf, device, step_size = 0.05 , n_iter = 20):\n",
    "\n",
    "        self.Nf = Nf\n",
    "        self.device = device\n",
    "        self.step_size = step_size\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def update(self, phy_lf, model):\n",
    "\n",
    "        x_new = torch.zeros(self.Nf, 2, dtype = torch.float32, device=self.device).uniform_(0, 1)\n",
    "        t_new = torch.zeros(self.Nf, 1, dtype = torch.float32, device=self.device).uniform_(0, 1)\n",
    "        x_t_new = torch.concatenate((x_new,t_new), axis = 1)\n",
    "        self.XTGrid = x_t_new\n",
    "            \n",
    "        x_data = self.XTGrid\n",
    "        samples = x_data.clone().detach().requires_grad_(True)\n",
    "    \n",
    "        for t in range(1, self.n_iter + 1):\n",
    "            grad = phy_lf(model, samples, self.device)\n",
    "            with torch.no_grad():\n",
    "                samples = samples + self.step_size * torch.sign(grad)\n",
    "                samples[:, 0] = torch.clamp(samples[:, 0], min=0, max=1)  \n",
    "                samples[:, 1] = torch.clamp(samples[:, 1], min=0, max=1)\n",
    "                samples[:, 2] = torch.clamp(samples[:, 2], min=0, max=1) \n",
    "            samples = samples.clone().detach().requires_grad_(True)\n",
    "        self.XTGrid = samples.detach()        \n",
    "\n",
    "\n",
    "class R3Sampler(nn.Module):\n",
    "    def __init__(self,Nf, fixed_uniform, device):\n",
    "        super(R3Sampler, self).__init__()\n",
    "        self.Nf = Nf\n",
    "        self.device = device\n",
    "        self.XTGrid = torch.tensor(copy.deepcopy(fixed_uniform), dtype=torch.float32, requires_grad=True).to(self.device)\n",
    "    \n",
    "    def update(self, loss_aver, loss_ele):\n",
    "        with torch.no_grad():\n",
    "            cho_i = loss_ele > loss_aver\n",
    "            cho_i = cho_i.to('cpu')\n",
    "            self.XTGrid = self.XTGrid[cho_i].detach()\n",
    "            need_n_sample = self.Nf-self.XTGrid.shape[0]\n",
    "            x_new = torch.zeros(need_n_sample, 2, dtype = torch.float32, device=self.device).uniform_(0, 1)\n",
    "            t_new = torch.zeros(need_n_sample, 1, dtype = torch.float32, device=self.device).uniform_(0, 1)\n",
    "            x_t_new = torch.concatenate((x_new,t_new), axis = 1)\n",
    "            self.XTGrid = torch.concatenate((self.XTGrid, x_t_new), axis = 0)\n",
    "            self.XTGrid = torch.tensor(self.XTGrid, dtype = torch.float32, device=self.device, requires_grad=True)\n",
    "    \n",
    "class PINN(nn.Module):\n",
    "    def __init__(self,k , c , t, exact_XYT, exact_u, space_domain, time_domain, Layers, N0, Nb, Nf, \n",
    "                 Activation = nn.Tanh(), \n",
    "                 model_name = \"PINN.model\", device = 'cpu',\n",
    "                  display_freq = 100, samp = 'fixed' ):\n",
    "        \n",
    "        super(PINN, self).__init__()\n",
    "        \n",
    "        \n",
    "        LBs = [space_domain[0], time_domain[0]]\n",
    "        UBs = [space_domain[1], time_domain[1]]\n",
    "        \n",
    "        self.LBs = torch.tensor(LBs, dtype=torch.float32).to(device)\n",
    "        self.UBs = torch.tensor(UBs, dtype=torch.float32).to(device)\n",
    "        \n",
    "        self.Layers = Layers\n",
    "        self.in_dim  = Layers[0]\n",
    "        self.out_dim = Layers[-1]\n",
    "        self.Activation = Activation\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        x_init = torch.zeros(Nf, 2, dtype = torch.float32, device=self.device).uniform_(0, 1)\n",
    "        t_init = torch.zeros(Nf, 1, dtype = torch.float32, device=self.device).uniform_(0, 1)\n",
    "        x_t_init = torch.concatenate((x_init,t_init), axis = 1)\n",
    "        self.fixed_uniform = torch.tensor(x_t_init, dtype = torch.float32, device=self.device, requires_grad=True)\n",
    "        \n",
    "        self.N0 = N0\n",
    "        self.Nb = Nb\n",
    "        self.Nf = Nf\n",
    "        \n",
    "        self.t = t\n",
    "        self.exact_XYT = exact_XYT\n",
    "        self.exact_u = exact_u.reshape(-1,1)\n",
    "        \n",
    "        self.XT0, self.u0  = self.InitialCondition(self.LBs[0], self.UBs[0])\n",
    "        self.left, self.right, self.top, self.bottom = self.BoundaryCondition(self.LBs[0], self.UBs[0])\n",
    "        \n",
    "        self.XT0 = self.XT0.to(device)\n",
    "        self.u0 = self.u0.to(device) \n",
    "        \n",
    "        self.left = self.left.to(device) \n",
    "        self.right = self.right.to(device)\n",
    "        self.top = self.top.to(device) \n",
    "        self.bottom = self.bottom.to(device)\n",
    "        \n",
    "        self._nn = self.build_model()\n",
    "        self._nn.to(self.device)\n",
    "        self.Loss = torch.nn.MSELoss(reduction='mean')\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.display_freq = display_freq\n",
    "        \n",
    "        self.k = k\n",
    "        self.c = c\n",
    "        self.X_star = self.exact_XYT\n",
    "        self.method = samp\n",
    "        \n",
    "        self.r3_sample = R3Sampler(self.Nf, self.fixed_uniform, device)\n",
    "        self.las = LASSampler(self.Nf, fixed_uniform=self.fixed_uniform, device=self.device, L_iter = 1, beta = 0.2, tau=2e-3)\n",
    "        self.l_inf = L_INFSampler(self.Nf, device=self.device, step_size = 0.05 , n_iter = 20)\n",
    "        self.rad = RADSampler(Nf = self.Nf, device=self.device, k = self.k, c=self.c)\n",
    "        \n",
    "    \n",
    "    def build_model(self):\n",
    "        Seq = nn.Sequential()\n",
    "        for ii in range(len(self.Layers)-1):\n",
    "            this_module = nn.Linear(self.Layers[ii], self.Layers[ii+1])\n",
    "            nn.init.xavier_normal_(this_module.weight)\n",
    "            Seq.add_module(\"Linear\" + str(ii), this_module)\n",
    "            if not ii == len(self.Layers)-2:\n",
    "                Seq.add_module(\"Activation\" + str(ii), self.Activation)\n",
    "        return Seq\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        x = x.reshape((-1,self.in_dim))  \n",
    "#         x = 2*(x - self.LBs)/(self.UBs - self.LBs) - 1.0\n",
    "        return torch.reshape(self._nn.forward(x), (-1, self.out_dim))\n",
    "\n",
    "    def InitialCondition(self,LB, UB):\n",
    "        n_per_dim = int(np.round(np.sqrt(self.N0)))\n",
    "        t_in = torch.tensor([0]).float()\n",
    "        x_in = torch.linspace(LB, UB, n_per_dim).float()\n",
    "        y_in = torch.linspace(LB, UB, n_per_dim).float()\n",
    "        XYT_in = stacked_grid(x_in, y_in, t_in)\n",
    "        uv0 = exact_soln(XYT_in)\n",
    "        return XYT_in, uv0\n",
    "\n",
    "    def BoundaryPoints(self,nb, xb, yb, LB, UB, where = 'left'):\n",
    "        n_per_dim = int(np.round(np.sqrt(nb)))\n",
    "        if where in ['left', 'right']:\n",
    "            Xb = torch.tensor([xb]).float()\n",
    "            Yb = torch.linspace(LB, UB, n_per_dim).float()\n",
    "        else:\n",
    "            Yb = torch.tensor([yb]).float()\n",
    "            Xb = torch.linspace(LB, UB, n_per_dim).float()\n",
    "        Tb = torch.linspace(LB, UB, nb).float()\n",
    "        return stacked_grid(Xb,Yb,Tb)\n",
    "\n",
    "    \n",
    "    def BoundaryCondition(self, LB, UB):\n",
    "        nb = int(np.round(self.Nb/4))\n",
    "        XYTleft = self.BoundaryPoints(nb, LB, LB, LB, UB,'left')\n",
    "        XYTright = self.BoundaryPoints(nb, UB, LB, LB, UB, 'right')\n",
    "        XYTtop = self.BoundaryPoints(nb, LB, UB, LB, UB, 'top')\n",
    "        XYTbottom = self.BoundaryPoints(nb, LB, LB, LB, UB, 'bottom')\n",
    "        return XYTleft, XYTright, XYTtop, XYTbottom\n",
    "    \n",
    "    def ICLoss(self):\n",
    "        uv0_pred = self.forward(self.XT0)\n",
    "        loss = self.Loss(uv0_pred, self.u0.to(self.device))\n",
    "        return loss\n",
    "        \n",
    "    def BCLoss(self):\n",
    "        U_L, U_R, U_T, U_B = self.forward(self.left), self.forward(self.right), self.forward(self.top), self.forward(self.bottom)\n",
    "        ULx, URx, UTx, UBx = exact_soln(self.left).to(self.device), exact_soln(self.right).to(self.device), \\\n",
    "                             exact_soln(self.top).to(self.device), exact_soln(self.bottom).to(self.device)\n",
    "        return self.Loss(U_L, ULx) + self.Loss(U_R, URx) + \\\n",
    "               self.Loss(U_T, UTx) + self.Loss(U_B, UBx)\n",
    "    \n",
    "    def PhysicsLoss(self, XTGrid):\n",
    "        xyt = XTGrid.requires_grad_(True).to(self.device)\n",
    "        uv = self.forward(xyt)\n",
    "        u = uv[:,0]\n",
    "        v = uv[:,1]\n",
    "\n",
    "        u_grad = torch.autograd.grad(outputs=u, inputs=xyt, grad_outputs=torch.ones(u.shape).to(self.device), create_graph=True, allow_unused=True)[0]\n",
    "        ux = u_grad[:,0]\n",
    "        uy = u_grad[:,1]\n",
    "        ut = u_grad[:,2]\n",
    "        uxx = torch.autograd.grad(outputs=ux, inputs=xyt, create_graph=True, grad_outputs=torch.ones(u.shape).to(self.device),allow_unused=True)[0][:,0]\n",
    "        uyy = torch.autograd.grad(outputs=uy, inputs=xyt, create_graph=True, grad_outputs=torch.ones(u.shape).to(self.device),allow_unused=True)[0][:,1]\n",
    "\n",
    "        v_grad = torch.autograd.grad(outputs=v, inputs=xyt, grad_outputs=torch.ones(u.shape).to(self.device), create_graph=True, allow_unused=True)[0]\n",
    "        vx = v_grad[:,0]\n",
    "        vy = v_grad[:,1]\n",
    "        vt = v_grad[:,2]\n",
    "        vxx = torch.autograd.grad(outputs=vx, inputs=xyt, create_graph=True, grad_outputs=torch.ones(u.shape).to(self.device),allow_unused=True)[0][:,0]\n",
    "        vyy = torch.autograd.grad(outputs=vy, inputs=xyt, create_graph=True, grad_outputs=torch.ones(u.shape).to(self.device),allow_unused=True)[0][:,1]\n",
    "\n",
    "        loss2 = (ut + u*ux + v*uy - (1/Re)*(uxx + uyy))**2 + \\\n",
    "                (vt + u*vx + v*vy - (1/Re)*(vxx + vyy))**2\n",
    "        loss1 = loss2.mean()\n",
    "             \n",
    "        return loss1, loss2 \n",
    "\n",
    "    def Train(self, n_iters, weights=(1.0,1.0,1.0)):\n",
    "        params = list(self.parameters())\n",
    "        optimizer = optim.Adam(params, lr=1e-3)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 5000, gamma=0.9, last_epoch=-1)\n",
    "        min_loss = 999999.0\n",
    "        Training_Losses = [-10]*n_iters\n",
    "        Test_Losses = []\n",
    "        rel_error = [-10]*(1+n_iters//1000)\n",
    "        \n",
    "        for jj in range(n_iters):\n",
    "            Total_ICLoss = torch.tensor(0.0, dtype = torch.float32, device=self.device, requires_grad = True)\n",
    "            Total_BCLoss = torch.tensor(0.0, dtype = torch.float32, device=self.device, requires_grad = True)\n",
    "            Total_PhysicsLoss = torch.tensor(0.0, dtype = torch.float32, device=self.device, requires_grad = True)\n",
    "            \n",
    "            Total_ICLoss = Total_ICLoss + self.ICLoss()\n",
    "            Total_BCLoss = Total_BCLoss + self.BCLoss()\n",
    "            \n",
    "            if self.method =='r3':\n",
    "                if jj == 0:\n",
    "                    XTGrid = self.r3_sample.XTGrid\n",
    "                    XTGrid = torch.tensor(XTGrid, dtype = torch.float32, device=self.device, requires_grad=True) \n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        self.r3_sample.update(loss1, loss2)\n",
    "                        XTGrid = self.r3_sample.XTGrid\n",
    "                        XTGrid = torch.tensor(XTGrid, dtype = torch.float32, device=self.device, requires_grad=True) \n",
    "                        \n",
    "            elif self.method == 'las':\n",
    "                if jj == 0:\n",
    "                    XTGrid = self.las.XTGrid\n",
    "                    XTGrid = torch.tensor(XTGrid, dtype=torch.float32, requires_grad=True).to(self.device)\n",
    "                else:\n",
    "                    if self.las.cnt % 1 == 0:# 4,6,8,10 cnt = 4, \n",
    "                        self.las.update(cal_domain_grad, self._nn)\n",
    "                    XTGrid = self.las.XTGrid\n",
    "                    XTGrid = torch.tensor(XTGrid, dtype=torch.float32, requires_grad=True).to(self.device)\n",
    "                self.las.cnt += 1\n",
    "            \n",
    "            elif self.method =='l_inf':\n",
    "                    self.l_inf.update(cal_domain_grad, self._nn)\n",
    "                    XTGrid = self.l_inf.XTGrid\n",
    "                    XTGrid = torch.tensor(XTGrid, dtype=torch.float32, requires_grad=True).to(self.device)\n",
    "                \n",
    "            elif self.method =='rad':\n",
    "                    self.rad.update(self._nn)\n",
    "                    XTGrid = self.rad.XTGrid\n",
    "                    XTGrid = torch.tensor(XTGrid, dtype=torch.float32, requires_grad=True).to(self.device)   \n",
    "            \n",
    "            elif self.method =='fixed':\n",
    "                    XTGrid = torch.tensor(self.fixed_uniform, dtype = torch.float32, device=self.device, requires_grad=True) \n",
    "            \n",
    "            elif self.method =='random-r':\n",
    "                    x_new = torch.zeros(self.Nf, 2, dtype = torch.float32, device=self.device).uniform_(0, 1)\n",
    "                    t_new = torch.zeros(self.Nf, 1, dtype = torch.float32, device=self.device).uniform_(0, 1)\n",
    "                    x_t_new = torch.concatenate((x_new,t_new), axis = 1)\n",
    "                    XTGrid = torch.tensor(x_t_new, dtype = torch.float32, device=self.device, requires_grad=True) \n",
    "                \n",
    "            optimizer.zero_grad()    \n",
    "            loss1, loss2 = self.PhysicsLoss(XTGrid) # For r3 method, loss2 contains element-wise errors\n",
    "            \n",
    "            Total_PhysicsLoss = Total_PhysicsLoss + loss1\n",
    "            Total_Loss = weights[0]*Total_ICLoss + weights[1]*Total_BCLoss\\\n",
    "                        + weights[2]*Total_PhysicsLoss \n",
    "            \n",
    "            Total_Loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            if Total_Loss < min_loss:\n",
    "                torch.save(self._nn.state_dict(), \"../models/\"+self.method+'_'+str(len(self.Layers)-2)+'_'+str(self.Nf)+'.pt')\n",
    "                min_loss = float(Total_Loss)\n",
    "                    \n",
    "            Training_Losses[jj] = float(Total_Loss)\n",
    "            \n",
    "            if (jj+1) % self.display_freq == 0:\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.forward(self.X_star)\n",
    "                    outputs = outputs.reshape(-1,1)\n",
    "                    re = np.linalg.norm(self.exact_u.cpu()-outputs.cpu().detach()) / np.linalg.norm(self.exact_u.cpu().detach())\n",
    "                    rel_error[int((jj+1)/1000)] = float(re*100)\n",
    "                    \n",
    "                print(\"Iteration Number = {}\".format(jj+1))\n",
    "                print(\"\\tIC Loss = {}\".format(float(Total_ICLoss)))\n",
    "                print(\"\\tBC Loss = {}\".format(float(Total_BCLoss)))\n",
    "                print(\"\\tPhysics Loss = {}\".format(float(Total_PhysicsLoss)))\n",
    "                print(\"\\tTraining Loss = {}\".format(float(Total_Loss)))\n",
    "                print(\"\\tRelative L2 error (test) = {}\".format(float(re*100)))\n",
    "\n",
    "        return Training_Losses, rel_error\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "                \n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--nodes', type=int, default = 128, help='The number of nodes per hidden layer in the neural network')\n",
    "        parser.add_argument('--layers', type=int, default = 8, help='The number of hidden layers in the neural network')\n",
    "        parser.add_argument('--N0', type=int, default = 100, help='The number of points to use on the initial condition')\n",
    "        parser.add_argument('--Nb', type=int, default = 100, help='The number of points to use on the boundary condition')\n",
    "        parser.add_argument('--Nf', type=int, default = 1000, help='The number of collocation points to use')\n",
    "        parser.add_argument('--epochs', type=int, default = 200000, help='The number of epochs to train the neural network')\n",
    "        parser.add_argument('--method', type=str, default='r3', help='Sampling method') # fixed, random-r, rad, r3, l_inf, las \n",
    "        parser.add_argument('--model-name', type=str, default='PINN_model', help='File name to save the model')\n",
    "        parser.add_argument('--display-freq', type=int, default=1000, help='How often to display loss information')\n",
    "        parser.add_argument('-f')\n",
    "        args = parser.parse_args()\n",
    "\n",
    "\n",
    "        if not os.path.exists(\"../models/\"):\n",
    "            os.mkdir(\"../models/\")\n",
    "            \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        NHiddenLayers = args.layers\n",
    "        \n",
    "        boundaries = [0, 1]\n",
    "        t_domain = [0., 1.]\n",
    "\n",
    "\n",
    "        Nx, Ny, Nt = 100, 100, 20\n",
    "        X = torch.linspace(boundaries[0], boundaries[1], Nx).float()\n",
    "        Y = torch.linspace(boundaries[0], boundaries[1], Ny).float()\n",
    "        T = torch.linspace(t_domain[0], t_domain[1], Nt).float()\n",
    "        XYT = stacked_grid(X,Y,T)\n",
    "        Exact_U = exact_soln(XYT)\n",
    "    \n",
    "        Layers = [3] + [args.nodes]*NHiddenLayers + [2]\n",
    "        Activation = nn.Tanh()\n",
    "\n",
    "        k = 1\n",
    "        c = 1\n",
    "\n",
    "        repeat = [0, 1, 2, 3, 4]\n",
    "        for i in repeat:\n",
    "            pinn = PINN(  k = k,\n",
    "                          c = c,\n",
    "                          t= T,\n",
    "                          exact_XYT = XYT,\n",
    "                          exact_u = Exact_U,\n",
    "                          Layers = Layers,\n",
    "                          space_domain = boundaries,\n",
    "                          time_domain = t_domain,\n",
    "                          N0 = args.N0,\n",
    "                          Nb = args.Nb,\n",
    "                          Nf = args.Nf,\n",
    "                          Activation = Activation,\n",
    "                          device = device,\n",
    "                          model_name = \"../models/\" + args.model_name + \".model_\"+args.method+'_'+str(args.layers)+'_'+str(args.Nf)+'_'+str(i),\n",
    "                          display_freq = args.display_freq, samp = args.method )\n",
    "\n",
    "            Losses_train, Losses_rel_l2 = pinn.Train(args.epochs, weights = (1, 1, 1)) # initial, boundary, residual\n",
    "\n",
    "            torch.save(Losses_train, \"../models/\" + args.model_name + \".loss_\"+args.method+'_'+str(args.layers)+'_'+str(args.Nf)+'_'+str(i))\n",
    "            torch.save(Losses_rel_l2, \"../models/\" + args.model_name + \".rel_l2_\"+args.method+'_'+str(args.layers)+'_'+str(args.Nf)+'_'+str(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
